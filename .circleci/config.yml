version: 2.1
orbs:
  aws-eks: circleci/aws-eks@1.0.3
  aws-ecr: circleci/aws-ecr@6.15.3
  kubernetes: circleci/kubernetes@0.11.2
  slack: circleci/slack@4.1

commands:
  destroy-environment:
    steps:
      - run:
          name: Destroy environments
          when: on_fail
          command: |
            echo "DESTROY STACKS"
            echo "Destroyed ?????"

jobs:
#--------------------- Job 1 ----------------
  linting:
    docker:
      - image: python:3.7.3-stretch
    working_directory: ~/project
    steps:
      - checkout
      - run:
          name: checking paths
          command: |
            echo 'path is:' ${pwd}
            echo 'the path is:'
            pwd
            ls -la
            ls -la ~/project/.circleci/dockerstuff

      # Download and cache dependencies
      - restore_cache:
          keys:
            - v1-dependencies-{{ checksum "~/project/.circleci/dockerstuff/requirements.txt" }}
            # fallback to using the latest cache if no exact match is found
            - v1-dependencies-

      - run:
          name: install dependencies
          command: |
            cd ~/project/.circleci/dockerstuff
            python3 -m venv venv
            ls -l venv
            ls -l venv/bin
            . venv/bin/activate
            apt-get -y install make
            make --version
            # Install hadolint
            wget -O /bin/hadolint https://github.com/hadolint/hadolint/releases/download/v1.16.3/hadolint-Linux-x86_64 &&\
              chmod +x /bin/hadolint

      - save_cache:
            paths:
              - ./venv
            key: v1-dependencies-{{ checksum "~/project/.circleci/dockerstuff/requirements.txt" }}

      # run lint!
      - run:
          name: Run lint
          command: |
            cd ~/project/.circleci/dockerstuff
            . venv/bin/activate
            make install
            make lint
      - slack/notify:
          event: fail
          template: basic_fail_1

#--------------------- Job 2 ------------------

  check-cluster:
    executor: aws-eks/python3
    parameters:
      cluster-name:
        description: |
          Name of the EKS cluster
        type: string
    steps:
      - kubernetes/install
      - aws-eks/update-kubeconfig-with-authenticator:
          cluster-name: << parameters.cluster-name >>
      - run:
          name: Kluster info
          command: |
            kubectl get services
            #aws eks update-kubeconfig --name capstone --region us-east-1
            kubectl get nodes
      - slack/notify:
          event: fail
          template: basic_fail_1

      # Here's where you will add some code to rollback on failure
      - destroy-environment

#--------------------- Job 3 --------------
  deploy-application:
    executor: aws-eks/python3
    parameters:
      cluster-name:
        description: |
          Name of the EKS cluster
        type: string
      docker-image-name:
        description: |
          Name of the docker image to be deployed
        type: string
      aws-region:
        description: |
          AWS region
        type: string
      account-url:
        description: |
          Docker AWS ECR repository url
        type: string
      tag:
        description: |
          Docker image tag
        type: string
    steps:
      - checkout
      - run:
          name: Replace placeholders with values in deployment template
          command: |
            pwd
            cat k8s/deployment.tpl |\
            sed "s|DOCKER_IMAGE_NAME|<< parameters.docker-image-name >>|" |\
            sed "s|DOCKER_REPO_NAME|<< parameters.account-url >>|" |\
            sed "s|DOCKER_IMAGE_TAG|<< parameters.tag >>|" > k8s/deployment.yml; \
            cat k8s/namespace.yml
      - aws-eks/update-kubeconfig-with-authenticator:
          cluster-name: << parameters.cluster-name >>
          install-kubectl: true
          aws-region: << parameters.aws-region >>
      - kubernetes/create-or-update-resource:
          action-type: apply
          resource-file-path: "k8s/namespace.yml"
          show-kubectl-command: true
          namespace: capstone
      - kubernetes/create-or-update-resource:
          action-type: apply
          resource-file-path: "k8s/deployment.tpl"
          show-kubectl-command: true
          get-rollout-status: true
          resource-name: deployment/capstone
          namespace: capstone
      - kubernetes/create-or-update-resource:
          action-type: apply
          resource-file-path: "k8s/service.yml"
          show-kubectl-command: true
          namespace: capstone

#--------------------- Job 4 --------------------------

  test-application:
    executor: aws-eks/python3
    parameters:
      cluster-name:
        description: |
          Name of the EKS cluster
        type: string
      aws-region:
        description: |
          AWS region
        type: string
        default: ""
      expected-version-info:
        description: |
          Expected app version (this is used for testing that the
          correct version has been deployed)
        type: string
    steps:
      - aws-eks/update-kubeconfig-with-authenticator:
          cluster-name: << parameters.cluster-name >>
          install-kubectl: true
          aws-region: << parameters.aws-region >>
      - run:
          name: Wait for service to be ready
          command: |
            kubectl get pods
            kubectl get services
            sleep 30
            for attempt in {1..20}; do
              EXTERNAL_IP=$(kubectl get service demoapp | awk '{print $4}' | tail -n1)
              echo "Checking external IP: ${EXTERNAL_IP}"
              if [ -n "${EXTERNAL_IP}" ] && [ -z $(echo "${EXTERNAL_IP}" | grep "pending") ]; then
                break
              fi
              echo "Waiting for external IP to be ready: ${EXTERNAL_IP}"
              sleep 10
            done
            sleep 180
            curl -s --retry 10 "http://$EXTERNAL_IP" | grep "<< parameters.expected-version-info >>"

#--------------------- Job 5------------

workflows:
  main:
    jobs:
      - linting
      #- eksctl-install
         #requires:
         #  - eskctl-install   
      - check-cluster:
          cluster-name: capstone
          requires:
            - linting
         #   - eskctl-install

